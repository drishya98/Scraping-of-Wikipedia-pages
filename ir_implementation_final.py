# -*- coding: utf-8 -*-
"""ir_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12FdOF0Q5tJrsMzV8bwPkqsunH06CTKNG
"""

import matplotlib.pyplot as plt
import plotly.plotly as py
import plotly.tools as tls
import nltk
import urllib
#nltk.download('punkt')
#nltk.download('stopwords')
import bs4 as bs  #beautiful soup for scraping
import urllib.request  #opening particular url
import re #regex for understanding sentences

scraped_data = urllib.request.urlopen(input("Enter the wikipedia url you wish to summarize: "))  
article = scraped_data.read() #stores the completely scraped data

parsed_article = bs.BeautifulSoup(article,'lxml') #to parse XML and HTML using this library

paragraphs = parsed_article.find_all('p') #to find it enclosed in paragraph tags

article_text = ""

for p in paragraphs:  
    article_text += p.text
#print(article_text)
print("end")

article_text = re.sub(r'\[[0-9]*\]', ' ', article_text) #obviously do not need references so remove them using regex  
article_text = re.sub(r'\s+', ' ', article_text) #if unicode flag is not set, does whitespace matching. If set, match characters within escape sequence like rftv
#print(article_text)

#clean text and calculate weighted frequencies
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
#remember to plot graphs later
#graph of formatted article text and article text.

#tokenization of sentences to words
sentence_list = nltk.sent_tokenize(article_text)  
#print(sentence_list)

#finding weighted frequencies
stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}  
for word in nltk.word_tokenize(formatted_article_text):  
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
#print(word_frequencies)
for word in word_frequencies:
  print(word,word_frequencies[word])
#plot a graph of word and its frequency..

#weighted frequency
maximum_frequncy = max(word_frequencies.values())

n_word_frequencies = {}

for word in word_frequencies.keys():  
    n_word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
#for word in n_word_frequencies:
  #print(word,n_word_frequencies[word])
  
#plot a graph of n_word_freq and word_freq

#calculating sentence scores
sentence_scores = {}  
for sent in sentence_list:  
    for word in nltk.word_tokenize(sent.lower()):
        if word in n_word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = n_word_frequencies[word]
                else:
                    sentence_scores[sent] += n_word_frequencies[word]
#for sent in sentence_list:
  #print(sent,sentence_scores[sent])

import heapq 

n=int(input("Enter the number of sentences you want in the summary: "))
summary_sentences = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)
file = open("new.txt", "w") 
 

summary = ' '.join(summary_sentences)
print(summary)
file.write("The summary is: ")
file.write(summary)

file.close()

query=input("Enter search word: ")
count=0
with open("new.txt") as openfile:
    for line in openfile:
        for part in line.split():
            if query in part:
                count=count+1
print("The word has been found " + str(count) + " number of times")

file = open("new.txt", "r+")
text=file.read()
search_word=input("Enter search word: ")
if search_word in text:
    with open("random.txt1"+'.html', mode='wt', encoding='utf-8') as f:
        f.write(text.replace(search_word, '<span style="color: red">{}</span>'.format(search_word)))
else:
    print("The word is not in the text")
file.close()

import numpy as np

# Bokeh libraries
from bokeh.io import output_notebook
from bokeh.plotting import figure, show

# My word count data
article_texts = np.linspace(1, 10, 10)
summary_words = [259,504,701,940,1216,1381,1602,1793,1967,2098]
cumulative_words = np.cumsum(summary_words)

# Output the visualization directly in the notebook
output_notebook()

# Create a figure with a datetime type x-axis
fig = figure(title='text summarizer progress',
             plot_height=400, plot_width=700,
             x_axis_label='article_len', y_axis_label='summary_len',
             x_minor_ticks=2, y_range=(0, 4000),
             toolbar_location=None)

# The daily words will be represented as vertical bars (columns)
fig.vbar(x=article_texts, bottom=0, top=summary_words, 
         color='blue', width=0.75, 
         legend='word_token')

# The cumulative sum will be a trend line
fig.line(x=article_texts, y=summary_words, 
         color='gray', line_width=1,
         legend='Cumulative')

# Put the legend in the upper left corner
fig.legend.location = 'top_left'

# Let's check it out
show(fig)

from bokeh.io import output_file
from bokeh.plotting import figure, show

# My x-y coordinate data
x = [1, 2, 1]
y = [1, 1, 2]

# Output the visualization directly in the notebook
output_file('summary', title="summarization")

# Create a figure with no toolbar and axis ranges of [0,3]
fig = figure(title='My Coordinates',
             plot_height=300, plot_width=300,
             x_range=(0, 3), y_range=(0, 3),
             toolbar_location=None)

# Draw the coordinates as circles
fig.circle(x=x, y=y,
           color='green', size=10, alpha=0.5)

# Show plot
show(fig)